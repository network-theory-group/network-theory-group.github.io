% Encoding: UTF-8

@InProceedings{Hua2012,
  author    = {Hua, Nan and Lall, Ashwin and Li, Baochun and Xu, Jun},
  title     = {{Towards Optimal Error-estimating Codes Through the Lens of Fisher Information Analysis}},
  booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
  year      = {2012},
  series    = {SIGMETRICS '12},
  pages     = {125--136},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2254773},
  label = {gEEC},
  available = {https://dl.acm.org/citation.cfm?id=2254773},
  doi       = {10.1145/2254756.2254773},
  file      = {EEC/Hua and Lall - Towards Optimal Error-Estimating Codes through the.pdf},
  isbn      = {978-1-4503-1097-0},
  keywords  = {Fisher information, error estimating coding},
  location  = {London, England, UK},
  numpages  = {12},
  tag       = {EEC},
  url       = {http://doi.acm.org/10.1145/2254756.2254773},
}

@InProceedings{Hua2012a,
  author    = {Hua, Nan and Lall, Ashwin and Li, Baochun and Xu, Jun},
  title     = {{A Simpler and Better Design of Error Estimating Coding}},
  booktitle = {Proceedings of the IEEE INFOCOM},
  year      = {2012},
  pages     = {235--243},
  address   = {Orlando, FL, USA},
  month     = mar,
  publisher = {IEEE},
  label = {tug-of-war},
  abstract  = {We study error estimating codes with the goal of establishing better bounds for the theoretical and empirical overhead of such schemes. We explore the idea of using sketch data structures for this problem, and show that the tug-of-war sketch gives an asymptotically optimal solution. The optimality of our algorithms are proved using communication complexity lower bound techniques. We then propose a novel enhancement of the tug-of-war sketch that greatly reduces the communication overhead for realistic error rates. Our theoretical analysis and assertions are supported by extensive experimental evaluation.},
  available = {https://ieeexplore.ieee.org/document/6195624},
  doi       = {10.1109/INFCOM.2012.6195624},
  file      = {EEC/Nan Hua et al. - 2012 - A simpler and better design of error estimating co.pdf},
  isbn      = {978-1-4673-0775-8 978-1-4673-0773-4 978-1-4673-0774-1},
  language  = {en},
  tag       = {EEC},
  url       = {http://ieeexplore.ieee.org/document/6195624/},
  urldate   = {2019-01-07},
}

@InProceedings{Zhang2017,
  author     = {Zhang, Zhenghao and Kumar, Piyush},
  title      = {{mEEC: A Novel Error Estimation Code with Multi-dimensional Feature}},
  booktitle  = {Proceedings of the IEEE INFOCOM},
  year       = {2017},
  pages      = {1--9},
  address    = {Atlanta, GA, USA},
  month      = {May},
  publisher  = {IEEE},
  abstract   = {Error estimation code estimates the bit error ratio in the data bits, and is very useful in many applications, such as estimating the number of errors in a packet transmitted over a wireless link. In this paper, we propose a novel error estimation code, mEEC, that outperforms the existing code by more than 10\%-20\% depending on the packet sizes, as well as being more unbiased. mEEC is mainly based on the idea of grouping multiple blocks of sampled data bits into a super-block, thus creating a multi-dimensional feature, then compressing features into a single number, called the color, as the coded bits. Through an intelligent coloring scheme, the blocks in a super-block share the cost of covering low probability cases, which allows the decoder to recover the actual feature values from the color value even in the presence of error. mEEC also adopts a lightweight redistribution step, which is guided by the solution of an optimization problem and further reduces the estimation errors and bias. We also show that mEEC can be implemented within reasonable table storage size and low time complexity.},
  available  = {https://ieeexplore.ieee.org/document/8057134},
  doi        = {10.1109/INFOCOM.2017.8057134},
  file       = {EEC/Zhang and Kumar - 2017 - mEEC A novel error estimation code with multi-dim.pdf},
  isbn       = {978-1-5090-5336-0},
  language   = {en},
  label = {mEEC},
  shorttitle = {{mEEC}},
  tag        = {EEC},
  url        = {http://ieeexplore.ieee.org/document/8057134/},
  urldate    = {2019-01-07},
}

@InProceedings{Huang2014,
  author    = {Huang, Jiwei and Yang, Sen and Lall, Ashwin and Romberg, Justin and Xu, Jun and Lin, Chuang},
  title     = {{Error Estimating Codes for Insertion and Deletion Channels}},
  booktitle = {The 2014 ACM International Conference on Measurement and Modeling of Computer Systems},
  year      = {2014},
  series    = {SIGMETRICS '14},
  pages     = {381--393},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2591976},
  label = {idEEC},
  available = {https://dl.acm.org/citation.cfm?id=2591976},
  doi       = {10.1145/2591971.2591976},
  file      = {EEC/Huang et al. - 2014 - Error estimating codes for insertion and deletion .pdf},
  isbn      = {978-1-4503-2789-3},
  keywords  = {deletion channel, error estimating coding, insertion channel},
  location  = {Austin, Texas, USA},
  numpages  = {13},
  tag       = {EEC},
  url       = {http://doi.acm.org/10.1145/2591971.2591976},
}

@Misc{Candes2010,
  author       = {Emmanuel Cand`es},
  title        = {{Information Theory of Data Matrices: Recovery from Incomplete and Corrupted Entries}},
  howpublished = {\url{https://www.itsoc.org/conferences/schools/past-schools/na-school-2010/lecture-files/candes/Emmanuel-Candes-Lecture.pdf}},
  month        = aug,
  year         = {2010},
  pages = {150},
  available    = {https://www.itsoc.org/conferences/schools/past-schools/na-school-2010/lecture-files/candes/Emmanuel-Candes-Lecture.pdf},
  file         = {Lecture/Candès - 2010 - Information Theory of Data Matrices   Recovery fr.pdf},
  tag          = {Lecture},
  type = {Lecture},
}

@Misc{Kellis2005,
  author       = {Manolis Kellis},
  title        = {{MIT 6.096 – Algorithms for Computational Biology Lecture 5: Sequence Alignment and Dynamic Programming}},
  howpublished = {\url{https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-096-algorithms-for-computational-biology-spring-2005/lecture-notes/lecture5_newest.pdf}},
  pages = {124},
  available = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-096-algorithms-for-computational-biology-spring-2005/lecture-notes/lecture5_newest.pdf},
  file = {Lecture/Manolis Kellis - 2005 - MIT 6.096 – Algorithms for Computational Biology L.pdf},
  tag = {Lecture},
  year         = {2005},
type = {Lecture},
}


@book{Woodruff2014,
 author = {Woodruff, David P.},
 title = {Sketching As a Tool for Numerical Linear Algebra},
 journal = {Found. Trends Theor. Comput. Sci.},
 issue_date = {October 2014},
 volume = {10},
 number = {1\&\#8211;2},
 month = oct,
 year = {2014},
 issn = {1551-305X},
 pages = {1--157},
 numpages = {157},
 url = {http://dx.doi.org/10.1561/0400000060},
 file = {Book/Woodruff - 2014 - Sketching As a Tool for Numerical Linear Algebra.pdf},
 available = {https://researcher.watson.ibm.com/researcher/files/us-dpwoodru/wNow.pdf},
 tag = {Book},
 doi = {10.1561/0400000060},
 acmid = {2693652},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@book{hastie_elements_2009,
	edition = {2 edition},
	title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
	shorttitle = {The Elements of Statistical Learning},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates.},
	language = {English},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	tag = {book},
	available = {https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics-ebook/dp/B00475AS2E},
	month = aug,
	year = {2009},
	file = {Hastie et al. - 2009 - The Elements of Statistical Learning Data Mining,.pdf}
}

@article{wang_lsh_survey_2014,
author    = {Jingdong Wang and
               Heng Tao Shen and
               Jingkuan Song and
               Jianqiu Ji},
title     = {Hashing for Similarity Search: {A} Survey},
journal   = {CoRR},
volume    = {abs/1408.2927},
year      = {2014},
url       = {http://arxiv.org/abs/1408.2927},
archivePrefix = {arXiv},
file = {LSH/Wang et al. - 2014 - Hashing for Similarity Search A Survey.pdf},
tag = {lsh},
eprint    = {1408.2927},
timestamp = {Mon, 13 Aug 2018 16:49:05 +0200},
biburl    = {https://dblp.org/rec/bib/journals/corr/WangSSJ14},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Manku_near_dup_2007,
  author = {Manku, Gurmeet Singh and Jain, Arvind and Das Sarma, Anish},
  title = {Detecting Near-duplicates for Web Crawling},
  booktitle = {Proceedings of the 16th International Conference on World Wide Web},
  series = {WWW '07},
  year = {2007},
  isbn = {978-1-59593-654-7},
  file = {near-duplicates/Manku et al. - 2007 - Detecting near-duplicates for web crawling.pdf},
  tag = {near-duplicates-detection},
  label = {near-duplicates},
  comment = {This paper applies Charikar’s fingerprinting technique (simHash) to detect near-duplicates for web crawling. 
     It presents an algorithmic technique for identifying existing f-bit fingerprints that differ from a given fingerprint in 
     at most k bit-positions, for small k.},
  location = {Banff, Alberta, Canada},
  pages = {141--150},
  numpages = {10},
  url = {http://doi.acm.org/10.1145/1242572.1242592},
  doi = {10.1145/1242572.1242592},
  acmid = {1242592},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {fingerprint, hamming distance, near-duplicate, search, similarity, sketch, web crawl, web document},
}
@Article{Broder_MinWiseIndependent_2000,
  author     = {Broder, Andrei Z and Charikar, Moses and Frieze, Alan M and Mitzenmacher, Michael},
  title      = {Min-Wise Independent Permutations},
  journal    = {J. Comput. Syst. Sci.},
  year       = {2000},
  volume     = {60},
  number     = {3},
  pages      = {630--659},
  month      = jun,
  issn       = {0022-0000},
  acmid      = {348768},
  address    = {Orlando, FL, USA},
  doi        = {10.1006/jcss.1999.1690},
  label = {minHash},
  file       = {Hash/Broder et al. - 2000 - Min-Wise Independent Permutations.pdf},
  issue_date = {June 2000},
  numpages   = {30},
  publisher  = {Academic Press, Inc.},
  tag        = {minHash},
  url        = {http://dx.doi.org/10.1006/jcss.1999.1690},
}

@InProceedings{Charikar_SimilarityEstimationTechniques_2002,
  author    = {Charikar, Moses S.},
  title     = {Similarity Estimation Techniques from Rounding Algorithms},
  booktitle = {Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing},
  year      = {2002},
  series    = {STOC '02},
  pages     = {380--388},
  label = {simHash},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {509965},
  doi       = {10.1145/509907.509965},
  file      = {Hash/Charikar - 2002 - Similarity Estimation Techniques from Rounding Alg.pdf},
  isbn      = {1-58113-495-9},
  location  = {Montreal, Quebec, Canada},
  numpages  = {9},
  tag       = {simHash},
  url       = {http://doi.acm.org/10.1145/509907.509965},
}

@Misc{noauthor_acoustic_2018,
  title     = {Acoustic fingerprint},
  month     = nov,
  year      = {2018},
  note      = {Page Version ID: 870512438},
  abstract  = {An acoustic fingerprint is a condensed digital summary, a fingerprint, deterministically generated from an audio signal, that can be used to identify an  audio sample or quickly locate similar items in an audio database.Practical uses of acoustic fingerprinting include identifying songs, melodies, tunes, or advertisements; sound effect library management; and video file identification. Media identification using acoustic fingerprints can be used to monitor the use of specific musical works and performances on radio broadcast, records, CDs, streaming media and peer-to-peer networks. This identification has been used in copyright compliance, licensing, and other monetization schemes.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal   = {Wikipedia},
  language  = {en},
  url       = {https://en.wikipedia.org/w/index.php?title=Acoustic_fingerprint&oldid=870512438},
  tag = {fingerprint},
  urldate   = {2019-01-18},
}


@Article{Navarro_GuidedTourApproximate_2001,
  author     = {Navarro, Gonzalo},
  title      = {A Guided Tour to Approximate String Matching},
  journal    = {ACM Comput. Surv.},
  year       = {2001},
  volume     = {33},
  number     = {1},
  pages      = {31--88},
  month      = mar,
  issn       = {0360-0300},
  acmid      = {375365},
  address    = {New York, NY, USA},
  doi        = {10.1145/375360.375365},
  file       = {app_string_match/Navarro - 2001 - A Guided Tour to Approximate String Matching.pdf},
  issue_date = {March 2001},
  keywords   = {Levenshtein distance, edit distance, online string matching, text searching allowing errors},
  numpages   = {58},
  publisher  = {ACM},
  tag        = {app-string-match},
  url        = {http://doi.acm.org/10.1145/375360.375365},
}

@Article{Ukkonen_Algorithmsapproximatestring_1985,
  author   = {Esko Ukkonen},
  title    = {Algorithms for approximate string matching},
  journal  = {Information and Control},
  year     = {1985},
  volume   = {64},
  number   = {1},
  pages    = {100 - 118},
  issn     = {0019-9958},
  note     = {International Conference on Foundations of Computation Theory},
  abstract = {The edit distance between strings a1 … am and b1 … bn is the minimum cost s of a sequence of editing steps (insertions, deletions, changes) that convert one string into the other. A well-known tabulating method computes s as well as the corresponding editing sequence in time and in space O(mn) (in space O(min(m, n)) if the editing sequence is not required). Starting from this method, we develop an improved algorithm that works in time and in space O(s · min(m, n)). Another improvement with time O(s · min(m, n)) and space O(s · min(s, m, n)) is given for the special case where all editing steps have the same cost independently of the characters involved. If the editing sequence that gives cost s is not required, our algorithms can be implemented in space O(min(s, m, n)). Since s = O(max(m, n)), the new methods are always asymptotically as good as the original tabulating method. As a by-product, algorithms are obtained that, given a threshold value t, test in time O(t · min(m, n)) and in space O(min(t, m, n)) whether s ⩽ t. Finally, different generalized edit distances are analyzed and conditions are given under which our algorithms can be used in conjunction with extended edit operation sets, including, for example, transposition of adjacent characters.},
  doi      = {https://doi.org/10.1016/S0019-9958(85)80046-2},
  file     = {app_string_match/Ukkonen - 1985 - Algorithms for approximate string matching.pdf},
  tag      = {app_string_match},
  url      = {http://www.sciencedirect.com/science/article/pii/S0019995885800462},
}

@Article{Bentley_MultidimensionalBinarySearch_1975,
  author     = {Bentley, Jon Louis},
  title      = {Multidimensional Binary Search Trees Used for Associative Searching},
  journal    = {Commun. ACM},
  year       = {1975},
  volume     = {18},
  number     = {9},
  pages      = {509--517},
  month      = sep,
  issn       = {0001-0782},
  acmid      = {361007},
  address    = {New York, NY, USA},
  doi        = {10.1145/361002.361007},
  label = {k-d tree},
  file       = {data_structure/Bentley - 1975 - Multidimensional Binary Search Trees Used for Asso.pdf},
  issue_date = {Sept. 1975},
  keywords   = {associative retrieval, attribute, binary search trees, binary tree insertion, information retrieval system, intersection queries, key, nearest neighbor queries, partial match queries},
  numpages   = {9},
  publisher  = {ACM},
  tag        = {data-structure},
  tutorial   = {http://groups.csail.mit.edu/graphics/classes/6.838/S98/meetings/m13/kd.html},
  url        = {http://doi.acm.org/10.1145/361002.361007},
}

@InProceedings{Ji_SuperbitLocality_2012,
  author    = {Ji, Jianqiu and Li, Jianmin and Yan, Shuicheng and Zhang, Bo and Tian, Qi},
  title     = {Super-bit Locality-sensitive Hashing},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  year      = {2012},
  series    = {NIPS'12},
  pages     = {108--116},
  address   = {USA},
  label = { SB-LSH },
  file = {LSH/Ji et al. - Super-Bit Locality-Sensitive Hashing.pdf},
  tag = {data-structure},
  comment = {Add an orthogonalization step before inner product step for simHash. More precisely, it 
  orthogonalizes K random Gaussian vectors before using them to generate the fingerprint. It decreases 
  the variance of the Haming distance estimator.},
  publisher = {Curran Associates Inc.},
  acmid     = {2999147},
  location  = {Lake Tahoe, Nevada},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2999134.2999147},
}

@InProceedings{Vempala_Randomlyorientedk_2012,
  author    = {Santosh S. Vempala},
  title     = {{Randomly-oriented k-d Trees Adapt to Intrinsic Dimension}},
  booktitle = {IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 2012)},
  year      = {2012},
  editor    = {Deepak D'Souza and Telikepalli Kavitha and Jaikumar Radhakrishnan},
  volume    = {18},
  series    = {Leibniz International Proceedings in Informatics (LIPIcs)},
  pages     = {48--57},
  label = {k-d tree },
  file = {data_structure/Vempala - 2012 - Randomly-oriented k-d Trees Adapt to Intrinsic Dim.pdf},
  tag = {data-structure},
  comment = {Suggested by Prof. Santosh S. Vempala},
  address   = {Dagstuhl, Germany},
  publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  annote    = {Keywords: Data structures, Nearest Neighbors, Intrinsic Dimension, k-d Tree},
  doi       = {10.4230/LIPIcs.FSTTCS.2012.48},
  isbn      = {978-3-939897-47-7},
  issn      = {1868-8969},
  url       = {http://drops.dagstuhl.de/opus/volltexte/2012/3847},
  urn       = {urn:nbn:de:0030-drops-38470},
}

@InProceedings{Dasgupta_RandomProjectionTrees_2008,
  author    = {Dasgupta, Sanjoy and Freund, Yoav},
  title     = {Random Projection Trees and Low Dimensional Manifolds},
  booktitle = {Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing},
  year      = {2008},
  series    = {STOC '08},
  pages     = {537--546},
  address   = {New York, NY, USA},
  label = {Random Projection Tree},
  file = {data_structure/Dasgupta and Freund - 2008 - Random projection trees and low dimensional manifo.pdf},
  tag = {data-structure},
  comment = {Suggested by Prof. Santosh S. Vempala},
  publisher = {ACM},
  acmid     = {1374452},
  doi       = {10.1145/1374376.1374452},
  isbn      = {978-1-60558-047-0},
  keywords  = {curse of dimension, k-d tree, manifold, random projection},
  location  = {Victoria, British Columbia, Canada},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/1374376.1374452},
}

@InProceedings{Rakthanmanon_SearchingMiningTrillions_2012,
  author    = {Rakthanmanon, Thanawin and Campana, Bilson and Mueen, Abdullah and Batista, Gustavo and Westover, Brandon and Zhu, Qiang and Zakaria, Jesin and Keogh, Eamonn},
  title     = {Searching and Mining Trillions of Time Series Subsequences Under Dynamic Time Warping},
  booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2012},
  series    = {KDD '12},
  pages     = {262--270},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2339576},
  doi       = {10.1145/2339530.2339576},
  isbn      = {978-1-4503-1462-6},
  keywords  = {lower bounds, similarity search, time series},
  label = {similarity search},
  comment = {Suggested by Prof. Jim Xu},
  tag = {similarity},
  location  = {Beijing, China},
  file = {LSH/Rakthanmanon et al. - 2012 - Searching and Mining Trillions of Time Series Subs.pdf},
  numpages  = {9},
  url       = {http://doi.acm.org/10.1145/2339530.2339576},
}



@article{arya_optimal_nodate,
	title = {An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions},
	language = {en},
	author = {Arya, Sunil},
	pages = {33},
	file = {ann/Arya - An Optimal Algorithm for Approximate Nearest Neigh.pdf},
  tag = {ann},
  label = {Approximate Nearest Neighbor},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf},
}

@article{raginsky_locality-sensitive_nodate,
	title = {Locality-Sensitive Binary Codes from Shift-Invariant Kernels},
	abstract = {This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.},
	language = {en},
	author = {Raginsky, Maxim and Lazebnik, Svetlana},
	pages = {9},
	file = {ann/Raginsky and Lazebnik - Locality-Sensitive Binary Codes from Shift-Invaria.pdf},
    tag = {lsh},
  label = {Localilty Sensitive Hashing},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf},
}

@article{sproull_refinements_1991,
	title = {Refinements to nearest-neighbor searching ink-dimensional trees},
	volume = {6},
	issn = {0178-4617, 1432-0541},
	url = {http://link.springer.com/10.1007/BF01759061},
	doi = {10.1007/BF01759061},
	abstract = {This note presents a simplification and generalization of an algorithm for searching kdimensional trees for nearest neighbors reported by Friedman et al. I-3].If the distance between records is measured using Lz, the Euclidean norm, the data structure used by the algorithm to determine the bounds of the search space can be simplified to a single number. Moreover, because distance measurements in L2 are rotationally invariant, the algorithm can be generalized to allow a partition plane to have an arbitrary orientation, rather than insisting that it be perpendicular to a coordinate axis, as in the original algorithm. When a k-dimensional tree is built, this plane can be found from the principal eigenvector of the covariance matrix of the records to be partitioned. These techniques and others yield variants of k-dimensional trees customized for specific applications.},
	language = {en},
	number = {1-6},
	urldate = {2019-01-25},
	journal = {Algorithmica},
	author = {Sproull, Robert F.},
	month = jun,
	year = {1991},
	pages = {579--589},
	file = {ann/Sproull - 1991 - Refinements to nearest-neighbor searching ink-dime.pdf},
    tag = {ann},
  label = {refined k-d tree},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf},
}

@article{muja_scalable_nodate,
	title = {Scalable Nearest Neighbour Methods for High Dimensional Data},
	language = {en},
	author = {Muja, Marius},
	pages = {105},
	file = {ann/Muja - Scalable Nearest Neighbour Methods for High Dimens.pdf},
    tag = {ann},
  label = {Approximate Nearest Neighbour},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf},
}

@inproceedings{muja_fast_2012,
	address = {Toronto, Ontario, Canada},
	title = {Fast Matching of Binary Features},
	isbn = {978-1-4673-1271-4 978-0-7695-4683-4},
	url = {http://ieeexplore.ieee.org/document/6233169/},
	doi = {10.1109/CRV.2012.60},
	abstract = {There has been growing interest in the use of binary-valued features, such as BRIEF, ORB, and BRISK for efﬁcient local feature matching. These binary features have several advantages over vector-based features as they can be faster to compute, more compact to store, and more efﬁcient to compare. Although it is fast to compute the Hamming distance between pairs of binary features, particularly on modern architectures, it can still be too slow to use linear search in the case of large datasets. For vector-based features, such as SIFT and SURF, the solution has been to use approximate nearest-neighbor search, but these existing algorithms are not suitable for binary features. In this paper we introduce a new algorithm for approximate matching of binary features, based on priority search of multiple hierarchical clustering trees. We compare this to existing alternatives, and show that it performs well for large datasets, both in terms of speed and memory efﬁciency.},
	language = {en},
	urldate = {2019-01-25},
	booktitle = {2012 {Ninth} {Conference} on {Computer} and {Robot} {Vision}},
	publisher = {IEEE},
	author = {Muja, Marius and Lowe, David G.},
	month = may,
	year = {2012},
	pages = {404--410},
	file = {ann/Muja and Lowe - 2012 - Fast Matching of Binary Features.pdf},
     tag = {ann},
  label = {Approximate Nearest Neighbour},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@article{liu_investigation_nodate,
	title = {An Investigation of Practical Approximate Nearest Neighbor Algorithms},
	abstract = {This paper concerns approximate nearest neighbor searching algorithms, which have become increasingly important, especially in high dimensional perception areas such as computer vision, with dozens of publications in recent years. Much of this enthusiasm is due to a successful new approximate nearest neighbor approach called Locality Sensitive Hashing (LSH). In this paper we ask the question: can earlier spatial data structure approaches to exact nearest neighbor, such as metric trees, be altered to provide approximate answers to proximity queries and if so, how? We introduce a new kind of metric tree that allows overlap: certain datapoints may appear in both the children of a parent. We also introduce new approximate k-NN search algorithms on this structure. We show why these structures should be able to exploit the same randomprojection-based approximations that LSH enjoys, but with a simpler algorithm and perhaps with greater efﬁciency. We then provide a detailed empirical evaluation on ﬁve large, high dimensional datasets which show up to 31-fold accelerations over LSH. This result holds true throughout the spectrum of approximation levels.},
	language = {en},
	author = {Liu, Ting and Moore, Andrew W and Gray, Alexander and Yang, Ke},
	pages = {8},
	file = {ann/Liu et al. - An Investigation of Practical Approximate Nearest .pdf},
  tag = {ann},
  label = {Approximate Nearest Neighbour},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@inproceedings{beygelzimer_cover_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {Cover trees for nearest neighbor},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143857},
	doi = {10.1145/1143844.1143857},
	abstract = {We present a tree data structure for fast nearest neighbor operations in general npoint metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric’s structure yet maintains all performance properties of a navigating net [KL04a]. If the point set has a bounded expansion constant c, which is a measure of the intrinsic dimensionality (as deﬁned in [KR02]), the cover tree data structure can be constructed in O c6n log n time. Furthermore, nearest neighbor queries require time only logarithmic in n, in particular O c12 log n time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.},
	language = {en},
	urldate = {2019-01-25},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	author = {Beygelzimer, Alina and Kakade, Sham and Langford, John},
	year = {2006},
	pages = {97--104},
	file = {ann/Beygelzimer et al. - 2006 - Cover trees for nearest neighbor.pdf},
  tag = {ann},
  label = {Approximate Nearest Neighbour},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@article{muja_scalable_2014,
	title = {Scalable Nearest Neighbor Algorithms for High Dimensional Data},
	volume = {36},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6809191/},
	doi = {10.1109/TPAMI.2014.2321376},
	abstract = {For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of ﬁnding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we ﬁnd two algorithms to be the most efﬁcient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated conﬁguration procedure for ﬁnding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not ﬁt in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.},
	language = {en},
	number = {11},
	urldate = {2019-01-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Muja, Marius and Lowe, David G.},
	month = nov,
	year = {2014},
	pages = {2227--2240},
	file = {ann/Muja and Lowe - 2014 - Scalable Nearest Neighbor Algorithms for High Dime.pdf},
    tag = {ann},
  label = {Approximate Nearest Neighbour},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@inproceedings{kulis_kernelized_2009,
	address = {Kyoto},
	title = {Kernelized locality-sensitive hashing for scalable image search},
	isbn = {978-1-4244-4420-5},
	url = {http://ieeexplore.ieee.org/document/5459466/},
	doi = {10.1109/ICCV.2009.5459466},
	abstract = {Fast retrieval methods are critical for large-scale and data-driven vision applications. Recent work has explored ways to embed high-dimensional features or complex distance functions into a low-dimensional Hamming space where items can be efﬁciently searched. However, existing methods do not apply for high-dimensional kernelized data when the underlying feature embedding for the kernel is unknown. We show how to generalize locality-sensitive hashing to accommodate arbitrary kernel functions, making it possible to preserve the algorithm’s sub-linear time similarity search guarantees for a wide class of useful similarity functions. Since a number of successful image-based kernels have unknown or incomputable embeddings, this is especially valuable for image retrieval tasks. We validate our technique on several large-scale datasets, and show that it enables accurate and fast performance for example-based object classiﬁcation, feature matching, and content-based retrieval.},
	language = {en},
	urldate = {2019-01-25},
	booktitle = {2009 {IEEE} 12th {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Kulis, Brian and Grauman, Kristen},
	month = sep,
	year = {2009},
	pages = {2130--2137},
	file = {ann/Kulis and Grauman - 2009 - Kernelized locality-sensitive hashing for scalable.pdf},
    tag = {lsh},
  label = {locality-sensitive hashing},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@article{forssen_visual_nodate,
	title = {Visual Object Recognition},
	language = {en},
	author = {Forssén, Per-Erik},
	pages = {38},
	file = {ann/Forssén - Visual Object Recognition.pdf},
      tag = {ann},
  label = {Approximate Nearest Neighbour},
  comment = { the slides from https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@article{lv_multi-probe_nodate,
	title = {Multi-Probe LSH: Efﬁcient Indexing for High-Dimensional Similarity Search},
	abstract = {Similarity indices for high-dimensional data are very desirable for building content-based search systems for featurerich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A signiﬁcant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two diﬀerent high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time eﬃciency. To achieve the same search quality, multi-probe LSH has a similar timeeﬃciency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method, to achieve the same search quality, multi-probe LSH uses less query time and 5 to 8 times fewer number of hash tables.},
	language = {en},
	author = {Lv, Qin and Josephson, William and Wang, Zhe and Charikar, Moses and Li, Kai},
	pages = {12},
	file = {ann/Lv et al. - Multi-Probe LSH Efﬁcient Indexing for High-Dimens.pdf},
        tag = {lsh},
  label = {Locality-Sensitive Hashing},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@article{andoni_near-optimal_nodate,
	title = {Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions},
	language = {en},
	author = {Andoni, Alexandr and Indyk, Piotr},
	pages = {6},
	file = {ann/Andoni and Indyk - Near-Optimal Hashing Algorithms for Approximate Ne.pdf},
  tag = {ann},
  label = {Approximate Nearest Neighbour},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@inproceedings{bawa_lsh_2005,
	address = {Chiba, Japan},
	title = {LSH forest: self-tuning indexes for similarity search},
	isbn = {978-1-59593-046-0},
	shorttitle = {{LSH} forest},
	url = {http://portal.acm.org/citation.cfm?doid=1060745.1060840},
	doi = {10.1145/1060745.1060840},
	abstract = {We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search engines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images; peer-to-peer systems desire distributed similarity indexes with low communication cost. We propose an indexing scheme called LSH Forest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing (LSH), but improves upon previous designs by (a) eliminating the different data-dependent parameters for which LSH must be constantly hand-tuned, and (b) improving on LSH’s performance guarantees for skewed data distributions while retaining the same storage and query overhead. We show how to construct this index in main memory, on disk, in parallel systems, and in peer-to-peer systems. We evaluate the design with experiments on multiple text corpora and demonstrate both the self-tuning nature and the superior performance of LSH Forest.},
	language = {en},
	urldate = {2019-01-25},
	booktitle = {Proceedings of the 14th international conference on {World} {Wide} {Web}  - {WWW} '05},
	publisher = {ACM Press},
	author = {Bawa, Mayank and Condie, Tyson and Ganesan, Prasanna},
	year = {2005},
	pages = {651},
	file = {ann/Bawa et al. - 2005 - LSH forest self-tuning indexes for similarity sea.pdf},
        tag = {lsh},
  label = {Locality-Sensitive Hashing},
  comment = { https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@article{freidman_algorithm_1977,
	title = {An Algorithm for Finding Best Matches in Logarithmic Expected Time},
	volume = {3},
	issn = {00983500},
	url = {http://portal.acm.org/citation.cfm?doid=355744.355745},
	doi = {10.1145/355744.355745},
	abstract = {An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional-to 1ogN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods.},
	language = {en},
	number = {3},
	urldate = {2019-01-25},
	journal = {ACM Transactions on Mathematical Software},
	author = {Freidman, Jerome H. and Bentley, Jon Louis and Finkel, Raphael Ari},
	month = sep,
	year = {1977},
	pages = {209--226},
	file = {ann/Freidman et al. - 1977 - An Algorithm for Finding Best Matches in Logarithm.pdf},
  tag = {ann},
  label = {Nearest Neighbour},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@inproceedings{weiss_spectral_2008,
	address = {USA},
	series = {{NIPS}'08},
	title = {Spectral Hashing},
	isbn = {978-1-60560-949-2},
	url = {http://dl.acm.org/citation.cfm?id=2981780.2981999},
	abstract = {Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to efficiently calculate the code of a novel data-point. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art.},
	urldate = {2019-01-25},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Weiss, Yair and Torralba, Antonio and Fergus, Rob},
	year = {2008},
	pages = {1753--1760},
	file = {ann/Weiss et al. - 2008 - Spectral Hashing.pdf},
  tag = {hashing},
  label = {Spectral Hashing},
  comment = {https://www.cvl.isy.liu.se/education/graduate/VOR14/Lecture6.pdf}, 
}

@misc{muja_fast_2019,
	title = {Fast Library for Approximate Nearest Neighbors},
	copyright = {View license},
	url = {https://github.com/mariusmuja/flann},
	urldate = {2019-01-25},
	author = {Muja, Marius},
	month = jan,
	year = {2019},
	note = {original-date: 2011-03-28T18:45:55Z},
	file = {ann/flann_manual-1.8.4.pdf},
   tag = {hashing},
  label = {Open Source Library for Approximate Nearest Neighbors},
  comment = {<a href="https://github.com/mariusmuja/flann" target="_blank">FLANN</a>},  
}
@Article{Chakraborty_ApproximatingEditDistance_2018,
  author        = {Diptarka Chakraborty and Debarati Das and Elazar Goldenberg and Michal Kouck{\'{y}} and Michael E. Saks},
  title         = {Approximating Edit Distance Within Constant Factor in Truly Sub-Quadratic Time},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1810.03664},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1810-03664},
  eprint        = {1810.03664},
  file          = {ann/Chakraborty et al. - 2018 - Approximating Edit Distance Within Constant Factor.pdf},
  label         = {Approximating Edit Distance},
  tag           = {ann},
  timestamp     = {Sun, 09 Dec 2018 13:42:32 +0100},
  url           = {http://arxiv.org/abs/1810.03664},
}

@Article{Landau_IncrementalStringComparison_1998,
  author  = {Landau, G. and Myers, E. and Schmidt, J.},
  title   = {Incremental String Comparison},
  journal = {SIAM Journal on Computing},
  year    = {1998},
  volume  = {27},
  number  = {2},
  pages   = {557-582},
  doi     = {10.1137/S0097539794264810},
  eprint  = {https://doi.org/10.1137/S0097539794264810},
  file    = {ann/Landau et al. - 1998 - Incremental String Comparison.pdf},
  label   = {edit distance},
  tag     = {ann},
  url     = {https://doi.org/10.1137/S0097539794264810},
}

@TechReport{Agarwal_Estimatingnumberdifferences_2006,
  author      = {Agarwal, Sachin and Trachtenberg, Ari and Ari, Sachin Agarwal},
  title       = {Estimating the number of differences between remote sets},
  institution = {Center for Information and Systems Engineering},
  year        = {2006},
  comment     = {Recommended by Prof. Jim},
  file        = {set_diff/Agarwal and Trachtenberg - Estimating the number of differences between remot.pdf},
  publisher   = {Citeseer},
  tag         = {set-diff},
  timestamp   = {2019-02-13},
}

@Article{Mitzenmacher_RobustSetReconciliation_2018,
  author        = {Michael Mitzenmacher and Tom Morgan},
  title         = {Robust Set Reconciliation via Locality Sensitive Hashing},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1807.09694},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1807-09694},
  eprint        = {1807.09694},
  file          = {set_diff/Mitzenmacher and Morgan - 2018 - Robust Set Reconciliation via Locality Sensitive H.pdf},
  label         = {latest},
  tag           = {set-diff},
  timestamp     = {2019-02-13},
  url           = {http://arxiv.org/abs/1807.09694},
}

@Comment{jabref-meta: databaseType:bibtex;}
